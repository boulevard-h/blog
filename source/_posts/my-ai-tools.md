---
layout: blog
title: AI 工具链更新记录
date: 2025-07-15 11:32:47
categories: 环境搭建
---

这篇文章开始于2025年7月，今年AI工具的发展日新月异，几乎每个月都能接触到新的模型/工具，所以谨以此文记录一下我用过的工具

作为一个研究僧，我使用AI工具主要是两方面：AI coding与使用AI写本子/大作业等，当然，我也会使用AI读论文，但目前感觉想要深度理解一篇文章，还是要自己静下心来去看去写，AI只能起到一个比较细小的辅助作用，因此这不算一个主要的方面

以及，本文只记录较长期（按月为单位）使用过的工具，像豆包、Qwen这种

## 1. GPT系列

梦开始的地方，第一次接触到是在大三下学期的GPT-3，当时用来写一些大作业/小项目代码

不过因为IP+支付方式的原因，当时OpenAI官网用的不是很顺畅，所以用的是Poe平台，记得有段时间还买过淘宝上20块钱一周的号，这种应该是商家利用新号一周试用，批量用假邮箱和信用卡注册的

后续很长一段时间也是用GPT为主，辅助我的毕业论文/fuzz项目，不断用过4o、o1、o1-mini一系列模型，记得第一次见到带思考功能的o1模型的时候还是很惊艳的

## 2. VSCode Copilot

这个大概是在24年上半年接触的，相比于现在的Vibe Coding与Code Agent，当时的Copilot其实只是一个比较智能的语法补全而已，对于比较复杂的代码逻辑，还是要放到Web端的AI来写

## 3. DeepSeek/腾讯元宝

DeepSeek的出现在国内确实是引发了轰动，虽然在解决比较复杂问题方面的能力还是不如同时间OpenAI的最新模型，但是它的价格实在是香。阿里云、硅基流动等平台都部署了自己的模型API，接入到CherryStudio中和国外基本20刀一个月的定价比起来基本等于没花钱。而后面出的腾讯元宝则是完全不收费，还带了网络搜索功能，所以也是用了很长一段时间，不过有几个比较坑的点：

- 一个是众所周知的，元宝虽然不收费但是会泄露用户数据
- DS确实是我用过的模型里面，最啰嗦、讲话AI味最冲的模型
- 虽然有搜索功能，但是主要来源还是微信公众号，质量一言难尽

## 4. Cursor

这个应该是在25年1月开始接触的，用了基本有半年，确实是对AI编程的一种革新。之前对着web网页AI进行编程拷打的时候，反复复制粘贴比较麻烦不说，AI很难理解整个项目架构，很多细节人为去描述很麻烦。而Cursor这种可以自动索引和访问工作区所有代码的Agent操作起来就十分方便。

不过Cursor的槽点就是玩不起，期初是20刀的Pro，每个月500条高级模型快速请求；但是后面变成了不透明的计费方式，而且明显感知越来越少和慢了，需要开通更贵的Pro Plus和Ultra模式才能够获得原先Pro就能做到的体验。

**250716更新：**Cursor现在ban了大陆IP，说是Anthropic加强了打击力度，但是顺带连GPT和Gemini也不能用了。虽然现在把梯子改成TUN模式还能用，但下次加大力度是哪天呢？idn

## 5. Grok-3

马圣的Grok-3在做一些学术调研的时候真的很棒，Grok的联网搜索模式很多数据都是来源于arxiv、Springer这些学术网站，不会出现其他AI瞎编文献的情况，堪称写综述的神

## 6. Gemini

Gemini主打一个超长上下文问文件管理，印象最深刻的是把很多个几十上百页的项目本子为个Gemini，让Gemini生成一些比较简短的新本子，Gemini不仅可以完全利用这一大堆资料，还能够标注出每一段话引用自资料的哪里。而Gemini的联网搜索调研，虽然数据库没有Grok-3的那么学术化，但也算质量较高的。

## 7. Gemini-CLI

书接上回Cursor玩不起以后，恰好就遇到了谷歌免费大力推广Gemini-CLI。

优点：

- 免费的Gemini-2.5-Pro，而且给的额度真的很高，没怎么被限制过
- 比起Cursor，感觉持续化的工作能力更强，给一个详细任务的Prompt后会自动迭代尝试很长时间，不需要人一直看着（前提是你放心给他所有操作都自动进行无需人为许可

缺点：

- Gemini-2.5-Pro综合能力很强，但是在Coding方面还是比Claude差一些，一贯的啰嗦
- 使用起来还是没有IDE built-in的Agent强，比如查看Diff就没有Cursor方便

当然，在CLI Agent这块目前最强的还是Claude Code，但是考虑到Anthropic对IP的封号力度和价格还是不敢去充钱尝试，最近国产的Kimi-V2好像完全兼容Anthropic的API，有人折腾出了替换的方式，后面有空可以试试

## 8. Claude Code + Kimi-K2

Kimi这波的新模型学习手机圈玩起**强**行**兼**容了，不仅完全兼容了 Anthropic 的API格式，而且最忍俊不禁的是：即使Claude Code在请求API的时候使用的model_name是Claude-*，Kimi 也会接受并默默调用自己的模型返回

配置非常简单，和正常配置API一样导出环境变量就行，不会弄的话还有一堆脚本，一次性完成安装、跳过初始化、写入API环境变量到rc

Kimi官方虽然给了15元的API额度，但其速度则是按照充值金额计费的，免费的速度几乎没法用；硅基流动的API速度快一些，但是配置了以后使用CC会出问题（具体表现为Agent一直在兜圈子，例如listing files...重复很多遍，而Kimi官方APi则不会）

## 9. 自建知识库

有一个文档集，例如某个库最新版本的API文档，或者某个庞大开源系统的技术文档，想要让其作为知识库给LLM，以便针对这个文档集向LLM问问题。

### 1) 自建RAG

RAG，按照我浅薄的理解是这样工作的：

1. 将文档分块成多个trunk，每个trunk都用embedding模型进行向量化
2. 当用户输入一个query的时候，匹配query与每个trunk的相似度，返回其中的top K个trunk作为结果

而搭建RAG的平台很多，我选择了最方便的Dify，此外还有ragflow、llamaindex等，并且使用Qwen3作为Base模型，提示词则是让gemini给我生成了一段

在试用一段时间后，发现主要有两个问题：

1. RAG的第一步是分块，而分块的粗细粒度是一个需要讲究的trade-off，如果分块过小（例如Dify默认按照 `\n\n` 分块）则返回的上下文不够；如果分块过大（例如返回一整个文档）则慢且消耗token。ragflow和llamaindex据说拥有更加好的分块处理，但是分块终究是需要根据应用场景不断进行召回测试才能做到比较好的
2. Dify默认的RAG模板流程是：用户query - 将query作为输入直接查询RAG - 将query和RAG查询结果都给LLM - LLM生成最终回答。但是用户的query并不一定是精确的，一种较好的方法是让LLM自己生成查询语句（不过我懒得做了）

### 2) Google Drive + Gemini

说到复杂文档的问答，还是Gemini用起来舒服。但是在Gemini网页版一次一次提交文件看起来有点呆，后面发现Gemini可以和Google Drive云盘联动，而且Google AI Pro是送2TB空间的，所以尝试了一下把整个文档的文件夹传到Google Drive里边，然后直接在Google Drive召唤Gemini就可以提问了，比用Dify搭建的RAG好用不少

